{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_name_generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ladvien/gan_name_maker/blob/master/deep_name_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZRbbbZcah2C",
        "colab_type": "text"
      },
      "source": [
        "# Deep Name Generator\n",
        "\n",
        "This project is meant to be a proof-of-concept.  Showing \"organic\" first names can be generated using a [Generative Advasarial Network](https://en.wikipedia.org/wiki/Generative_adversarial_network). We are using a found dataset provided by [Hadley Wickham](http://hadley.nz/) at RStudio.\n",
        "\n",
        "The goal will be to vectorize each of the names in the following format:\n",
        "\n",
        "| a_0 | b_0 | c_0 | ... | z_9 | a_10 | etc |\n",
        "|-----|-----|-----|-----|-----|------|-----|\n",
        "|  1  |  0  |  0  | ... |  1  |  0   |  0  |\n",
        "|  0  |  0  |  1  | ... |  0  |  0   |  0  |\n",
        "\n",
        "Where the letter is the one-hot encoded representation of a character and the number the placeholder in string.\n",
        "\n",
        "For example, the name `Abby` would be represented with the following vector.\n",
        "\n",
        "| a_0 | ... | b_1 | ... | b_2 | ... | y_3 |\n",
        "|-----|-----|-----|-----|-----|-----|-----|\n",
        "|  1  | ... |  1  | ... |  1  | ... |  1  |\n",
        "\n",
        "Given Wickham's dataset also includes:\n",
        "\n",
        "* `year`\n",
        "* `percent_[popularity]`\n",
        "* `sex`\n",
        "\n",
        "It may be interesting to add these as additional features to allow the model to learn first name contexts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SP6BUY6cuYN",
        "colab_type": "text"
      },
      "source": [
        "## Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tec9znYkaY11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgVaYPw9eJwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Engineering parameters.\n",
        "pad_character       = '~'\n",
        "allowed_chars       = f'abcdefghijklmnopqrstuvwxyz{pad_character}'\n",
        "len_allow_chars     = len(allowed_chars)\n",
        "max_name_length     = 10 \n",
        "\n",
        "templated_df = pd.DataFrame()\n",
        "\n",
        "# Create the dataframe.\n",
        "for i in range(max_name_length):\n",
        "  for char in allowed_chars:\n",
        "    templated_df[char + '_' + str(i)] = 0\n",
        "\n",
        "# Show the first and last ten columns.\n",
        "templated_df.columns.tolist()[0:10] + templated_df.columns.tolist()[-10:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv0_k9iXZYBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/hadley/data-baby-names.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHb-MMFrc1cE",
        "colab_type": "text"
      },
      "source": [
        "## Examine the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kYYmmG3Zc2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/data-baby-names/baby-names.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDoqTO7RZl7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XWl2Nocc709",
        "colab_type": "text"
      },
      "source": [
        "### Name Popularity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDR4Fpn2ZoJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.sort_values(by = 'percent', ascending = False).head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBkzBILReGcF",
        "colab_type": "text"
      },
      "source": [
        "## Preparing Dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBeUh-WqdC9g",
        "colab_type": "text"
      },
      "source": [
        "## Vectorizing Names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlEQLBHk211a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names = df['name'].str.lower().unique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY9qnwwL3FTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_unique_names = len(names)\n",
        "print(f'Found total of {num_unique_names} unique names')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1REecJVdEs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: This code could be made tons more performant by refactoring. \n",
        "#       Right now, it's a slow as a snail on salt.\n",
        "\n",
        "def vectorize_name(name, max_name_length, allowed_chars, pad_character):\n",
        "  tmp = []\n",
        "\n",
        "  # Standardize\n",
        "  name = name.lower()\n",
        "\n",
        "  # Pad the name if needed.\n",
        "  while len(name) < max_name_length:\n",
        "    name += pad_character\n",
        "\n",
        "  feature_index = 0\n",
        "\n",
        "  # Create the pandas series object.\n",
        "  name_vector = pd.Series()\n",
        "  \n",
        "  # Loop through all placeholders\n",
        "  for feature_index in range(max_name_length):\n",
        "      # Loop through all allowed charcters\n",
        "      for allowed_char in allowed_chars:\n",
        "\n",
        "          # Create a feature for each allowed character by its placeholder (e.g., \"j_4\")\n",
        "          feature_name = allowed_char + '_' + str(feature_index)\n",
        "\n",
        "          # If the name has a character in the placeholder, flag it as true.\n",
        "          if name[feature_index] == allowed_char:\n",
        "            name_vector[feature_name] = 1\n",
        "          else:\n",
        "            name_vector[feature_name] = 0\n",
        "  return name_vector\n",
        "\n",
        "name_vector = vectorize_name('adam', max_name_length, allowed_chars, pad_character)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAzZIduwvbH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "name_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad66c88q1X6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del df\n",
        "# Create a 'test' feature vector to force building the dataframe feature names.\n",
        "df = pd.DataFrame([vectorize_name('test', max_name_length, allowed_chars, pad_character)])\n",
        "\n",
        "for name in names:\n",
        "  print(name)\n",
        "  name_vector = vectorize_name(name, max_name_length, allowed_chars, pad_character)\n",
        "  \n",
        "  name_vector['name'] = name\n",
        "  df = df.append(name_vector, ignore_index = True, sort = False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MhCsw0B3uv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('vectorized_names.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cPi9CWbCIIu",
        "colab_type": "text"
      },
      "source": [
        "# GAN\n",
        "Working off the following Keras GAN Example:\n",
        "https://towardsdatascience.com/gan-by-example-using-keras-on-tensorflow-backend-1a6d515a60d0\n",
        "\n",
        "Another good article on GANS and text generation\n",
        "https://becominghuman.ai/generative-adversarial-networks-for-text-generation-part-1-2b886c8cab10\n",
        "\n",
        "And a good one on transformers (Attention is All You Need)\n",
        "https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04\n",
        "\n",
        "https://medium.com/datadriveninvestor/generative-adversarial-network-gan-using-keras-ce1c05cfdfd3\n",
        "\n",
        "https://machinelearningmastery.com/practical-guide-to-gan-failure-modes/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFUm6NVwlBhl",
        "colab_type": "text"
      },
      "source": [
        "# Prepared Data\n",
        "If you'd like to skip to the fun part, I've vectorized the names already."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSe1MuytmITK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/Ladvien/gan_name_maker"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As904FlmBKIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzgoHm5JlNsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('./gan_name_maker/vectorized_names.csv')\n",
        "cols = list(df)\n",
        "\n",
        "# Move the name column to the beginning.\n",
        "cols.insert(0, cols.pop(cols.index('name')))\n",
        "df = df.loc[:, cols]\n",
        "\n",
        "# Drop the yucky columns.\n",
        "df.drop('Unnamed: 0', axis = 1, inplace = True)\n",
        "\n",
        "# Sort values by name\n",
        "df.sort_values(by = 'name', ascending = True, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6grgIAkrlcJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Vectorized data has {df.shape[0]} samples and {df.shape[1]} features.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7e2lps4ltuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slMz61VmoJQT",
        "colab_type": "text"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9ParNeMoLTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Input\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.callbacks import History \n",
        "\n",
        "# Personal tools.\n",
        "!pip install git+https://github.com/Ladvien/ladvien_ml.git\n",
        "from ladvien_ml import FeatureModel\n",
        "\n",
        "fm = FeatureModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpTYkEHePumr",
        "colab_type": "text"
      },
      "source": [
        "# Setup Weights and Biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR5XPZTXPzl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade wandb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1gpmkV2dttN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wandb login 186e8a3df54055bf2ce699bf0e3f5320c9bb29e6\n",
        "import wandb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e2F6h59bsxU",
        "colab_type": "text"
      },
      "source": [
        "# Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zOYovMJbvFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters\n",
        "optimizer_name        = 'adadelta'\n",
        "learning_rate         = 1.0\n",
        "epochs                = 1000\n",
        "batch_size            = 32\n",
        "num_samples           = 10\n",
        "dropout               = 0.2\n",
        "generator_inputs      = 800\n",
        "width_modifier        = 0.5 # Multiplied by inputs to give neurons in dense.\n",
        "\n",
        "# Input shape will be the number of possible characters times \n",
        "# the maximum name length allowed.\n",
        "vectorized_name_length = len_allow_chars * max_name_length\n",
        "\n",
        "params = {\n",
        "    'epoch': epochs,\n",
        "    'batch_size': batch_size,\n",
        "    'optimizer_name': optimizer_name,\n",
        "    'generator_inputs': generator_inputs,\n",
        "    'num_samples_per_step': num_samples,\n",
        "    'allowed_chars': allowed_chars,\n",
        "    'max_name_length': max_name_length,\n",
        "    'dropout': dropout,\n",
        "    'width_modifier': width_modifier\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JegWy5nWpGiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wandb.init(project = 'deep_name_generator',\n",
        "           config = params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU9MGepjeiwY",
        "colab_type": "text"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSsQieXf5rrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator(input_shape, optimizer, dropout = 0.1, width_modifier = 0.5):\n",
        "  \n",
        "  D = Sequential()\n",
        "  \n",
        "  # Input layer\n",
        "  input_layer_width = input_shape\n",
        "  D.add(Dense(input_layer_width, input_shape = (input_layer_width,), activation = 'relu'))\n",
        "  D.add(Dropout(dropout))\n",
        "  \n",
        "  # First Hidden Layer\n",
        "  first_layer_width = input_shape * width_modifier\n",
        "  D.add(Dense(first_layer_width, activation = 'relu'))\n",
        "  D.add(Dropout(dropout))\n",
        "\n",
        "  # Second Hidden Layer\n",
        "  second_layer_width = input_shape * width_modifier\n",
        "  D.add(Dense(second_layer_width, activation = 'relu'))\n",
        "  D.add(Dropout(dropout))\n",
        "\n",
        "  # Third Hidden Layer\n",
        "  third_layer_width = input_shape * width_modifier\n",
        "  D.add(Dense(third_layer_width, activation = 'relu'))\n",
        "  D.add(Dropout(dropout))\n",
        " \n",
        "  # Output\n",
        "  D.add(Dense(1, activation = 'sigmoid'))\n",
        "  D._name = 'discriminator'\n",
        "  D.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "  D.summary()\n",
        "\n",
        "  return D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jCZu1O_ehC5",
        "colab_type": "text"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ETyLikQemYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator(num_inputs, output_shape, optimizer, dropout = 0.1, width_modifier = 0.5):\n",
        "\n",
        "  G = Sequential()\n",
        "\n",
        "  # Input layer\n",
        "  input_layer_width = num_inputs\n",
        "  G.add(Dense(input_layer_width, input_shape = (input_layer_width,), activation = 'relu'))\n",
        "  G.add(Dropout(dropout))\n",
        "  \n",
        "  # First Hidden Layer\n",
        "  first_layer_width = num_inputs * width_modifier\n",
        "  G.add(Dense(first_layer_width, activation = 'relu'))\n",
        "  G.add(Dropout(dropout))\n",
        "\n",
        "  # Second Hidden Layer\n",
        "  second_layer_width = num_inputs * width_modifier\n",
        "  G.add(Dense(second_layer_width, activation = 'relu'))\n",
        "  G.add(Dropout(dropout))\n",
        "\n",
        "  # Third Hidden Layer\n",
        "  third_layer_width = num_inputs * width_modifier\n",
        "  G.add(Dense(third_layer_width, activation = 'relu'))\n",
        "  G.add(Dropout(dropout))\n",
        "  \n",
        "  # Output layer \n",
        "  G.add(Dense(output_shape, activation = 'sigmoid'))\n",
        "  G._name = 'generator'\n",
        "  G.compile(optimizer = optimizer, loss = 'categorical_crossentropy')\n",
        "  G.summary()\n",
        "  return G"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktd9H1xn_rPH",
        "colab_type": "text"
      },
      "source": [
        "# GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BSwzfdm_uWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_gan(D, G, g_inputs):\n",
        "    D.trainable = False\n",
        "    gan_input = Input(shape = (g_inputs,))\n",
        "    x = G(gan_input)\n",
        "    gan_output = D(x)\n",
        "    gan = Model(inputs = gan_input, outputs = gan_output)\n",
        "    gan.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop')\n",
        "    return gan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YXdtuIWhMS4",
        "colab_type": "text"
      },
      "source": [
        "# Compile\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIc1C_mFhNHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select optimizer.\n",
        "optimizer = fm.select_optimizer(optimizer_name, learning_rate)\n",
        "\n",
        "# Generator\n",
        "G = generator(generator_inputs, vectorized_name_length, optimizer, dropout = dropout)\n",
        "\n",
        "# Discriminator\n",
        "D = discriminator(vectorized_name_length, optimizer, dropout = dropout)\n",
        "\n",
        "# Build GAN\n",
        "GAN = create_gan(D, G, generator_inputs)\n",
        "\n",
        "GAN._name = 'GAN'\n",
        "GAN.compile(loss='binary_crossentropy', optimizer = optimizer, metrics=['accuracy'])\n",
        "GAN.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yd6XZx7vUub",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBnekWKcvOm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Randomize inputs.\n",
        "df = df.sample(df.shape[0])\n",
        "\n",
        "# Create target label.\n",
        "df['real'] = 1\n",
        "\n",
        "# Drop the 'name' and 'real' columns.\n",
        "X = df.iloc[:,1:-1]\n",
        "\n",
        "# Get target\n",
        "y = df.iloc[:,-1:]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kITi3_SZUEi",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htuRNht5ZWj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def retrieve_names_from_sparse_matrix(generated_names, pad_character):\n",
        "  retrieved_names = []\n",
        "  for name_index in range(len(generated_names)):\n",
        "    generated_name = ''\n",
        "    name_array = generated_names[name_index]\n",
        "    for char_index in range(max_name_length):\n",
        "      \n",
        "      # Get A index.\n",
        "      first_letter_index = (char_index * len_allow_chars)\n",
        "      last_letter_index = (char_index * len_allow_chars + len_allow_chars)\n",
        "      char_vector = list(name_array[first_letter_index:last_letter_index])\n",
        "      \n",
        "      char = allowed_chars[char_vector.index(max(char_vector))]\n",
        "\n",
        "      if char == pad_character:\n",
        "        break\n",
        "        \n",
        "      generated_name += char\n",
        "\n",
        "    # print(generated_name) \n",
        "    retrieved_names.append(generated_name)\n",
        "\n",
        "  return retrieved_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z8UEyLdZSDp",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3LgSUEctLVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the data\n",
        "batch_count = x_train.shape[0] / batch_size\n",
        "\n",
        "for e in range(1, epochs + 1):\n",
        "    print(f'Epoch: {e}')\n",
        "    for step in range(batch_size):\n",
        "        # Generate noise as input to initialize generator.\n",
        "        noise = np.random.normal(0, 1, [batch_size, generator_inputs])\n",
        "        \n",
        "        # Generate fake names from noise.\n",
        "        generated_names = G.predict(noise)\n",
        "        \n",
        "        # Get a random set of  real images\n",
        "        real_names = x_train.iloc[np.random.randint(low = 0, high = x_train.shape[0], size = batch_size),:]\n",
        "\n",
        "        #Construct different batches of  real and fake data \n",
        "        X = np.concatenate([real_names, generated_names])\n",
        "        \n",
        "        # Labels for generated and real data (first four rows are real)\n",
        "        y_labels = np.zeros(2 * batch_size)\n",
        "        y_labels[:batch_size] = 1\n",
        "        \n",
        "        # Pre-train discriminator on fake and real data before starting the GAN. \n",
        "        D.trainable = True\n",
        "        D.train_on_batch(X, y_labels)\n",
        "        \n",
        "        # Tricking the noised input of the Generator as real data\n",
        "        noise = np.random.normal(0, 1, [batch_size, generator_inputs])\n",
        "        y_gen = np.ones(batch_size)\n",
        "        \n",
        "        # During the training of GAN, the weights of discriminator should be \n",
        "        # fixed. We can enforce that by setting the trainable flag.\n",
        "        D.trainable = False\n",
        "        \n",
        "        # Train the GAN by alternating the training of the Discriminator \n",
        "        # and training the chained GAN model with Discriminator’s weights \n",
        "        # frozen.\n",
        "        GAN_score = GAN.train_on_batch(noise, y_gen)\n",
        "        print(f'GAN loss: {GAN_score[0]}')\n",
        "        D_score = D.evaluate(X, y_labels)\n",
        "        print(f'Disc. loss: {D_score}')\n",
        "\n",
        "    # Log to Weights and Biases\n",
        "    wandb.log({'GAN Loss': GAN_score[0], \n",
        "              'epoch': e, \n",
        "              'discriminator_loss': D_score[0],\n",
        "              'discriminator_accuracy': D_score[1]\n",
        "    })\n",
        "\n",
        "    # Make Generator inputs.\n",
        "    noise = np.random.normal(0, 1, [num_samples, generator_inputs])\n",
        "\n",
        "    # Generate fake names from noise.\n",
        "    generated_names = G.predict(noise)\n",
        "    retrieved_names = retrieve_names_from_sparse_matrix(generated_names, pad_character)\n",
        "\n",
        "    # Save generated names.\n",
        "    table = wandb.Table(columns=['Name', 'Epoch'])\n",
        "\n",
        "    for name in retrieved_names:\n",
        "      table.add_data(name, e)\n",
        "    wandb.log({\"generated_names\": table})\n",
        "          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h9EJhlxeTIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make Generator inputs.\n",
        "noise = np.random.normal(0, 1, [20, generator_inputs])\n",
        "\n",
        "# Generate fake names from noise.\n",
        "generated_names = G.predict(noise)\n",
        "retrieve_names_from_sparse_matrix(generated_names, pad_character)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TclR2EaJkmdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}